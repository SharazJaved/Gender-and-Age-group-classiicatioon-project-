{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training and buiilding the Gender and age group classification model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step #1**: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "cv2_version: 4.9.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from shutil import copyfile\n",
    "from scipy.stats import skew, kurtosis\n",
    "########################################################\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "###########################################\n",
    "# This is a simple keras or tensorflow.keras library import for CNN \n",
    "from keras.models import Sequential,load_model,Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense,Input,Dropout,BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "# Use scikit-learn to grid search the batch size and epochs\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras_tuner import Hyperband\n",
    "import keras_tuner as kt\n",
    "##################################################\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from keras.callbacks import Callback\n",
    "####################################\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# This is a simple keras library import for CNN \n",
    "print(\"cv2_version:\",cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelname: exp01_genderAge_model\n",
      "project_path: d:\\deep learning\\project for deep learning\\Gender_Age-group_classification_project\n",
      "result_filepath: d:\\deep learning\\project for deep learning\\Gender_Age-group_classification_project/experiments/experiment_01/result/\n"
     ]
    }
   ],
   "source": [
    "#saveMLmodelfile_name=\"MLarch#04_genderAge_modelep25\"\n",
    "%store -r saveMLmodelfile_name\n",
    "print(\"modelname:\", saveMLmodelfile_name)\n",
    "%store -r project_path\n",
    "print(\"project_path:\", project_path)\n",
    "%store -r result_filepath\n",
    "print(\"result_filepath:\", result_filepath)\n",
    "\n",
    "\n",
    "#figure_dirPath='./model/'+str(saveMLmodelfile_name)+\"/\"+\"/figure/\"\n",
    "# Ensure the train and test directories exist\n",
    "#os.makedirs(figure_dirPath, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step #5**: preparation input face_image ( UTKFace_traindataset and UTKFace_traindataset)  of the  CNN  model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) normalization of image and paths for your train and val dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for your dataset\n",
    "train_dirPath = project_path+\"/dataset/UTKFace_Processed_Traindataset\"\n",
    "val_dirPath= project_path+\"/dataset/UTKFace_Processed_Valdataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_image(image):\n",
    "    # Convert to floating-point values #img_float = (image.astype(np.float32))\n",
    "    # Normalize pixel values to [0, 1]\n",
    "    normalized_image = (image.astype(np.float32)) / 255.0\n",
    "\n",
    "    return normalized_image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) normalization of image of  UTKFace_traindataset and UTKFace_traindataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca2d9e72f354b54925a1f643533dce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43420 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total no of train_faceImage:  43420\n",
      "sample of train_faceImage:  [[[0.19607843 0.3137255  0.42745098]\n",
      "  [0.15294118 0.2784314  0.39215687]\n",
      "  [0.16470589 0.29803923 0.4117647 ]\n",
      "  ...\n",
      "  [0.3372549  0.48235294 0.6156863 ]\n",
      "  [0.3019608  0.4392157  0.57254905]\n",
      "  [0.32156864 0.45882353 0.5921569 ]]\n",
      "\n",
      " [[0.20392157 0.32156864 0.43529412]\n",
      "  [0.16078432 0.28627452 0.4       ]\n",
      "  [0.14901961 0.28235295 0.39607844]\n",
      "  ...\n",
      "  [0.3372549  0.48235294 0.6156863 ]\n",
      "  [0.32156864 0.45882353 0.5921569 ]\n",
      "  [0.3254902  0.4627451  0.59607846]]\n",
      "\n",
      " [[0.20392157 0.32156864 0.42745098]\n",
      "  [0.1764706  0.30588236 0.40784314]\n",
      "  [0.14901961 0.28627452 0.3882353 ]\n",
      "  ...\n",
      "  [0.3529412  0.49411765 0.63529414]\n",
      "  [0.35686275 0.49411765 0.627451  ]\n",
      "  [0.33333334 0.47058824 0.6039216 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.00784314 0.01568628 0.01568628]\n",
      "  [0.01568628 0.02352941 0.02352941]\n",
      "  [0.01960784 0.02745098 0.02745098]\n",
      "  ...\n",
      "  [0.2784314  0.4117647  0.5529412 ]\n",
      "  [0.18039216 0.28235295 0.4       ]\n",
      "  [0.11372549 0.20784314 0.3019608 ]]\n",
      "\n",
      " [[0.00784314 0.01568628 0.01568628]\n",
      "  [0.01568628 0.02352941 0.02352941]\n",
      "  [0.01960784 0.02745098 0.02745098]\n",
      "  ...\n",
      "  [0.22352941 0.34901962 0.4862745 ]\n",
      "  [0.13333334 0.22745098 0.3372549 ]\n",
      "  [0.09803922 0.1882353  0.27450982]]\n",
      "\n",
      " [[0.00784314 0.01568628 0.01568628]\n",
      "  [0.01568628 0.02352941 0.02352941]\n",
      "  [0.01960784 0.02745098 0.02745098]\n",
      "  ...\n",
      "  [0.12941177 0.25490198 0.39215687]\n",
      "  [0.07450981 0.16470589 0.26666668]\n",
      "  [0.09411765 0.1764706  0.2627451 ]]]\n"
     ]
    }
   ],
   "source": [
    "train_faceImage=[]\n",
    "\n",
    "for img in tqdm(os.listdir(train_dirPath)): \n",
    "    train_faceImage.append(normalize_image(np.array(cv2.imread(str(train_dirPath)+\"/\"+str(img),-1))))\n",
    "\n",
    "total_train_faceImage=len(train_faceImage)\n",
    "print(\"total no of train_faceImage: \", len(train_faceImage))\n",
    "print(\"sample of train_faceImage: \",train_faceImage[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "868e09d790744299bf2fb719c769cfb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total of the val_faceImage:  4124\n",
      "sample of val_faceImage:  [[[0.10196079 0.14509805 0.27058825]\n",
      "  [0.2901961  0.33333334 0.45882353]\n",
      "  [0.18039216 0.22745098 0.36862746]\n",
      "  ...\n",
      "  [0.9529412  0.8352941  0.6745098 ]\n",
      "  [0.972549   0.85490197 0.67058825]\n",
      "  [0.9647059  0.84705883 0.654902  ]]\n",
      "\n",
      " [[0.09803922 0.14117648 0.26666668]\n",
      "  [0.20392157 0.24705882 0.37254903]\n",
      "  [0.1764706  0.22352941 0.3647059 ]\n",
      "  ...\n",
      "  [0.9490196  0.83137256 0.67058825]\n",
      "  [0.9607843  0.84705883 0.67058825]\n",
      "  [0.9529412  0.84705883 0.6509804 ]]\n",
      "\n",
      " [[0.12156863 0.16470589 0.2901961 ]\n",
      "  [0.14509805 0.1882353  0.3137255 ]\n",
      "  [0.14509805 0.19215687 0.33333334]\n",
      "  ...\n",
      "  [0.9372549  0.8235294  0.67058825]\n",
      "  [0.94509804 0.8392157  0.6666667 ]\n",
      "  [0.94509804 0.84313726 0.65882355]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.68235296 0.56078434 0.38431373]\n",
      "  [0.34509805 0.21960784 0.08235294]\n",
      "  [0.24313726 0.09803922 0.04313726]\n",
      "  ...\n",
      "  [0.5411765  0.40784314 0.2901961 ]\n",
      "  [0.6117647  0.46666667 0.3882353 ]\n",
      "  [0.70980394 0.5568628  0.5019608 ]]\n",
      "\n",
      " [[0.75686276 0.63529414 0.45882353]\n",
      "  [0.42352942 0.29803923 0.16078432]\n",
      "  [0.1764706  0.03137255 0.        ]\n",
      "  ...\n",
      "  [0.5254902  0.3882353  0.37254903]\n",
      "  [0.87058824 0.7176471  0.7490196 ]\n",
      "  [0.76862746 0.6117647  0.65882355]]\n",
      "\n",
      " [[0.6745098  0.5529412  0.3764706 ]\n",
      "  [0.5803922  0.45490196 0.31764707]\n",
      "  [0.20392157 0.05882353 0.00392157]\n",
      "  ...\n",
      "  [0.5411765  0.40392157 0.44313726]\n",
      "  [0.8352941  0.68235296 0.7647059 ]\n",
      "  [0.6156863  0.45490196 0.56078434]]]\n"
     ]
    }
   ],
   "source": [
    "val_faceImage=[]\n",
    "\n",
    "for img in tqdm(os.listdir(val_dirPath)): \n",
    "    val_faceImage.append(normalize_image(np.array(cv2.imread(str(val_dirPath)+\"/\"+str(img),-1))))\n",
    "\n",
    "print(\"total of the val_faceImage: \", len(val_faceImage))\n",
    "print(\"sample of val_faceImage: \",val_faceImage[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step #6**: preparation output label (gender and age group) of the  CNN  model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) extract the  gender and age annotation   form UTKFace_traindataset and UTKFace_valdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9520af5311314521b1e7b014a4399f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43420 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenght of UTKFace_trainDataset gender: 43420\n",
      "train_gender list: [0 0 0 ... 1 1 1]\n",
      "lenght of UTKFace_trainDataset aga: 43420\n",
      "train_age list: [10 10 10 ...  9  9  9]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_age = []\n",
    "train_gender = [] \n",
    "\n",
    "for img in tqdm(os.listdir(train_dirPath)):\n",
    "  train_age.append(np.array(img.split(\"_\")[0],np.uint64))\n",
    "  train_gender.append(np.array(img.split(\"_\")[1],np.uint64))\n",
    "\n",
    "train_age = np.array(train_age,np.uint64)\n",
    "train_gender = np.array(train_gender,np.uint64)\n",
    "\n",
    "print(\"lenght of UTKFace_trainDataset gender:\",len(train_gender))\n",
    "print(\"train_gender list:\",train_gender)\n",
    "print(\"lenght of UTKFace_trainDataset aga:\", len(train_age))\n",
    "print(\"train_age list:\",train_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf85a2b170b4baeb9982739f8479ef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4124\n",
      "[10 10 10 ...  9  9  9]\n",
      "4124\n",
      "[0 0 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "val_age = []\n",
    "val_gender = [] \n",
    "\n",
    "for img in tqdm(os.listdir(val_dirPath)):\n",
    "  val_age.append(np.array(img.split(\"_\")[0],np.uint64))\n",
    "  val_gender.append(np.array(img.split(\"_\")[1],np.uint64))\n",
    "\n",
    "val_age = np.array(val_age,np.uint64)\n",
    "val_gender = np.array(val_gender,np.uint64)\n",
    "\n",
    "print(len(val_age))\n",
    "print(val_age)\n",
    "print(len(val_gender))\n",
    "print(val_gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b)  one hot encoding for the train_age  and val_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 7, 13, 20, 27, 33, 41, 46, 51, 57]\n",
      "[6, 12, 19, 26, 32, 40, 45, 50, 56, 65]\n",
      "['3-6', '7-12', '13-19', '20-26', '27-32', '33-40', '41-45', '46-50', '51-56', '57-65']\n",
      "Stored 'age_classes' (list)\n"
     ]
    }
   ],
   "source": [
    "%store -r lower_agelimit\n",
    "%store -r upper_agelimit\n",
    "\n",
    "print (lower_agelimit)\n",
    "print (upper_agelimit)\n",
    "\n",
    "# Define the age classes\n",
    "# ['1-2', '3-12', '13-19', '20-24', '25-27', '28-30', '31-33', '34-36', '37-40', '41-45', '46-50', '51-55', '56-63', '64-70', '71-116']\n",
    "age_classes = []\n",
    "\n",
    "for lower, upper in zip(lower_agelimit, upper_agelimit):\n",
    "    formatted_range = f\"{lower}-{upper}\"\n",
    "    age_classes.append(formatted_range)\n",
    "\n",
    "print((age_classes))\n",
    "%store age_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def map_age_to_class(age):\n",
    "    age = int(age)\n",
    "    if age >= lower_agelimit[0] and age <= upper_agelimit[0]:\n",
    "        return age_classes[0]#'1-7'\n",
    "    elif age >= lower_agelimit[1] and age <= upper_agelimit[1]:\n",
    "        return age_classes[1]#'8-12'\n",
    "    elif age >= lower_agelimit[2] and age <= upper_agelimit[2]:\n",
    "        return age_classes[2]#'13-18'\n",
    "    elif age >= lower_agelimit[3] and age <= upper_agelimit[3]:\n",
    "        return age_classes[3]#'19-24'\n",
    "    elif age >= lower_agelimit[4] and age <= upper_agelimit[4]:\n",
    "        return age_classes[4]#'25-30'\n",
    "    elif age >= lower_agelimit[5] and age <= upper_agelimit[5]:\n",
    "        return age_classes[5]#'31-36'\n",
    "    elif age >= lower_agelimit[6] and age <= upper_agelimit[6]:\n",
    "        return age_classes[6]#'37-41'\n",
    "    elif age >= lower_agelimit[7] and age <= upper_agelimit[7]:\n",
    "        return age_classes[7]#'42-47'\n",
    "    elif age >= lower_agelimit[8] and age <= upper_agelimit[8]:\n",
    "        return age_classes[8]#'48-53'\n",
    "    # elif age >= lower_agelimit[9] and age <= upper_agelimit[9]:\n",
    "    #     return age_classes[9]#'54-58'\n",
    "    # elif age >= lower_agelimit[10] and age <= upper_agelimit[10]:\n",
    "    #     return age_classes[10]#'59-64'\n",
    "    # elif age >= lower_agelimit[11] and age <= upper_agelimit[11]:\n",
    "    #     return age_classes[11]#'65-70'\n",
    "    else:\n",
    "        return age_classes[9]#'71-116'\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43420\n",
      "10\n",
      "train_age_encoded:\n",
      " [[0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 1]\n",
      " ...\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 1]]\n",
      "4124\n",
      "10\n",
      "val_age_encoded:\n",
      " [[0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 1]\n",
      " ...\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Map ages to age classes\n",
    "val_age_classes_mapped = [map_age_to_class(age) for age in val_age]\n",
    "train_age_classes_mapped = [map_age_to_class(age) for age in train_age]\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Reshape the mapped age classes into a 2D array\n",
    "val_age_classes_mapped_2d = np.array(val_age_classes_mapped).reshape(-1, 1)\n",
    "train_age_classes_mapped_2d = np.array(train_age_classes_mapped).reshape(-1, 1)\n",
    "\n",
    "# Fit and transform the OneHotEncoder on the reshaped array of train age \n",
    "val_age_encoded = (encoder.fit_transform(val_age_classes_mapped_2d)).astype(int)\n",
    "train_age_encoded = (encoder.fit_transform(train_age_classes_mapped_2d)).astype(int)\n",
    "\n",
    "# Print the result\n",
    "print(len(train_age_encoded))\n",
    "print(len(train_age_encoded[0]))\n",
    "\n",
    "print(\"train_age_encoded:\\n\",(train_age_encoded))\n",
    "\n",
    "\n",
    "# Print the result\n",
    "print(len(val_age_encoded))\n",
    "print(len(val_age_encoded[0]))\n",
    "\n",
    "print(\"val_age_encoded:\\n\",(val_age_encoded))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_faceImage = np.array(train_faceImage)\n",
    "train_gender = np.array(train_gender,np.uint64)\n",
    "val_faceImage = np.array(val_faceImage)\n",
    "val_gender = np.array(val_gender,np.uint64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43420, 100, 100, 3)\n",
      "(43420,)\n",
      "(43420, 10)\n",
      "(4124, 100, 100, 3)\n",
      "(4124,)\n",
      "(4124, 10)\n"
     ]
    }
   ],
   "source": [
    "print(train_faceImage.shape)\n",
    "print(train_gender.shape)\n",
    "print(train_age_encoded.shape)\n",
    "print(val_faceImage.shape)\n",
    "print(val_gender.shape)\n",
    "print(val_age_encoded.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step #7**: CNN model development for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Define the CNN model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_CNNmodel(hp):\n",
    "    # Define the model architecture\n",
    "    #,kernel_regularizer=l2(0.001),\n",
    "    input = Input(shape = (100,100,3), name=\"face_image\")\n",
    "\n",
    "    conv2d_1 =Conv2D(32, (3,3), activation='relu',name=\"conv2d_1\")(input)\n",
    "    maxpool2d_1 = MaxPooling2D(pool_size = (2,2),name=\"maxpool2d_1\") (conv2d_1)\n",
    "    BN_01=BatchNormalization(name=\"BN_01\") (maxpool2d_1)\n",
    "\n",
    "    conv2d_2 =Conv2D(64, (3,3), activation='relu',name=\"conv2d_2\")(BN_01)\n",
    "    maxpool2d_2 = MaxPooling2D(pool_size = (2,2),name=\"maxpool2d_2\") (conv2d_2)\n",
    "    BN_02=BatchNormalization(name=\"BN_02\") (maxpool2d_2)\n",
    "\n",
    "    ###\n",
    "    conv2d_3 =Conv2D(128, (3,3), activation='relu',name=\"conv2d_3\")(maxpool2d_2)\n",
    "    maxpool2d_3 = MaxPooling2D(pool_size = (2,2),name=\"maxpool2d_3\") (conv2d_3)\n",
    "    BN_03=BatchNormalization(name=\"BN_03\") (maxpool2d_3)\n",
    "\n",
    "    ####\n",
    "    conv2d_4 =Conv2D(256, (3,3), activation='relu', name=\"conv2d_4\")(BN_03)\n",
    "    maxpool2d_4 = MaxPooling2D(pool_size = (2,2), name=\"maxpool2d_4\") (conv2d_4)\n",
    "    BN_04=BatchNormalization(name=\"BN_04\") (maxpool2d_4)\n",
    "\n",
    "    flatten = Flatten(name=\"flatten\")(BN_04)\n",
    "    hp_units1 = hp.Int('units1', min_value=32, max_value=512, step=32)\n",
    "    FC_dense_1 = Dense(units=hp_units1,activation='relu', name=\"FC_dense_1\")(flatten)\n",
    "    FC_dense_2 = Dense(units=hp_units1,activation='relu', name=\"FC_dense_2\")(FC_dense_1)\n",
    "    BN_05=BatchNormalization(name=\"BN_05\") (FC_dense_2)\n",
    "    FC_dense_3 = Dense(units=hp_units1,activation='relu', name=\"FC_dense_3\")(BN_05)\n",
    "    BN_06=BatchNormalization(name=\"BN_06\") (FC_dense_3)\n",
    "\n",
    "    ################################\n",
    "    #gender part \n",
    "    FC64_dense_gender = Dense(128,activation='relu', name=\"FC64_dense_gender\")(BN_06)\n",
    "    drop_1 = Dropout(0.5, name=\"drop_1\")(FC64_dense_gender)\n",
    "    #FC32_dense_gender = Dense(32,activation='relu', name=\"FC32_dense_gender\")(FC64_dense_gender)\n",
    "    FC16_dense_gender = Dense(64,activation='relu', name=\"FC16_dense_gender\")(drop_1)\n",
    "    BN_07=BatchNormalization(name=\"BN_07\") (FC16_dense_gender)\n",
    "    FC16_dense_gender_01 = Dense(32,activation='relu', name=\"FC16_dense_gender_01\")(BN_07)\n",
    "    BN_08=BatchNormalization(name=\"BN_08\") (FC16_dense_gender_01)\n",
    "\n",
    "    output_gender = Dense(1,activation=\"sigmoid\", name=\"gender_output\")(BN_07)\n",
    "\n",
    "    ################################\n",
    "    #age part \n",
    "    FC128_dense_age_01 = Dense(128,activation='relu', name=\"FC128_dense_age_01\")(BN_06)\n",
    "    BN_09=BatchNormalization(name=\"BN_09\") (FC128_dense_age_01)\n",
    "    FC128_dense_age_02 = Dense(128,activation='relu', name=\"FC128_dense_age_02\")(BN_09)\n",
    "    FC64_dense_age_03 = Dense(128,activation='relu', name=\"FC64_dense_age_03\")(FC128_dense_age_02)\n",
    "    BN_10=BatchNormalization(name=\"BN_10\") (FC64_dense_age_03)\n",
    "    FC64_dense_age_04 = Dense(64,activation='relu', name=\"FC64_dense_age_04\")(FC64_dense_age_03)\n",
    "    BN_11=BatchNormalization(name=\"BN_11\") (FC64_dense_age_04)\n",
    "    FC32_dense_age_05 = Dense(32,activation='relu', name=\"FC32_dense_age_05\")(FC64_dense_age_04)\n",
    "    BN_12=BatchNormalization(name=\"BN_12\") (FC32_dense_age_05)\n",
    "    output_age = Dense(10,activation=\"softmax\", name=\"age_output\")(BN_12)\n",
    "\n",
    "    output=[output_gender,output_age]\n",
    "    DLmodel = Model(inputs=input,outputs=output, name=\"DL_model\")\n",
    "    DLmodel.compile(optimizer=\"adam\",loss=[\"binary_crossentropy\",\"categorical_crossentropy\"],metrics=['accuracy'])\n",
    "    \n",
    "    return DLmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #CNNmodel_dirPath=\"model/CNNmodel/\"\n",
    "# # Ensure the train and test directories exist\n",
    "# #os.makedirs(CNNmodel_dirPath, exist_ok=True)\n",
    "\n",
    "# import pydotplus.graphviz as gv\n",
    "\n",
    "# # Specify the GraphViz executable path\n",
    "# gv.find_graphviz()\n",
    "\n",
    "# # Now try to plot the model\n",
    "# from keras.utils import plot_model\n",
    "# model=create_CNNmodel()\n",
    "# model.summary()\n",
    "# plot_model(model, show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from d:\\deep learning\\project for deep learning\\Gender_Age-group_classification_project/experiments/experiment_01/result//keras_tuner_result\\gender_age_facail\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Create a Keras Tuner Hyperband tuner instance\n",
    "tuner = kt.Hyperband(\n",
    "    create_CNNmodel,\n",
    "    objective= kt.Objective(\"val_gender_output_accuracy\", direction=\"max\"),\n",
    "    max_epochs=10,\n",
    "    factor=3,\n",
    "    directory= result_filepath+'/keras_tuner_result',\n",
    "    project_name='gender_age_facail'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 19 Complete [00h 26m 20s]\n",
      "val_gender_output_accuracy: 0.9296799302101135\n",
      "\n",
      "Best val_gender_output_accuracy So Far: 0.9468961954116821\n",
      "Total elapsed time: 18h 06m 28s\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Search for the best hyperparameters\n",
    "tuner.search(train_faceImage,[train_gender,train_age_encoded], epochs=10, validation_data=(val_faceImage,[val_gender,val_age_encoded]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 5: Retrieve the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_model = tuner.hypermodel.build(best_hps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.src.engine.functional.Functional object at 0x000002BB30B66550>\n",
      "Results summary\n",
      "Results in d:\\deep learning\\project for deep learning\\Gender_Age-group_classification_project/experiments/experiment_01/result//keras_tuner_result\\gender_age_facail\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_gender_output_accuracy\", direction=\"max\")\n",
      "\n",
      "Trial 0015 summary\n",
      "Hyperparameters:\n",
      "units1: 288\n",
      "tuner/epochs: 10\n",
      "tuner/initial_epoch: 4\n",
      "tuner/bracket: 2\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0012\n",
      "Score: 0.9468961954116821\n",
      "\n",
      "Trial 0012 summary\n",
      "Hyperparameters:\n",
      "units1: 288\n",
      "tuner/epochs: 4\n",
      "tuner/initial_epoch: 2\n",
      "tuner/bracket: 2\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0007\n",
      "Score: 0.9374393820762634\n",
      "\n",
      "Trial 0014 summary\n",
      "Hyperparameters:\n",
      "units1: 64\n",
      "tuner/epochs: 4\n",
      "tuner/initial_epoch: 2\n",
      "tuner/bracket: 2\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0002\n",
      "Score: 0.9369544386863708\n",
      "\n",
      "Trial 0013 summary\n",
      "Hyperparameters:\n",
      "units1: 448\n",
      "tuner/epochs: 4\n",
      "tuner/initial_epoch: 2\n",
      "tuner/bracket: 2\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0003\n",
      "Score: 0.931134819984436\n",
      "\n",
      "Trial 0016 summary\n",
      "Hyperparameters:\n",
      "units1: 480\n",
      "tuner/epochs: 4\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.9306498765945435\n",
      "\n",
      "Trial 0018 summary\n",
      "Hyperparameters:\n",
      "units1: 224\n",
      "tuner/epochs: 4\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.9296799302101135\n",
      "\n",
      "Trial 0017 summary\n",
      "Hyperparameters:\n",
      "units1: 512\n",
      "tuner/epochs: 4\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 1\n",
      "tuner/round: 0\n",
      "Score: 0.9221629500389099\n",
      "\n",
      "Trial 0007 summary\n",
      "Hyperparameters:\n",
      "units1: 288\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 0.9219204783439636\n",
      "\n",
      "Trial 0003 summary\n",
      "Hyperparameters:\n",
      "units1: 448\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 0.9177982807159424\n",
      "\n",
      "Trial 0002 summary\n",
      "Hyperparameters:\n",
      "units1: 64\n",
      "tuner/epochs: 2\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 2\n",
      "tuner/round: 0\n",
      "Score: 0.9171109795570374\n"
     ]
    }
   ],
   "source": [
    "print(best_model)\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"DL_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " face_image (InputLayer)     [(None, 100, 100, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 98, 98, 32)           896       ['face_image[0][0]']          \n",
      "                                                                                                  \n",
      " maxpool2d_1 (MaxPooling2D)  (None, 49, 49, 32)           0         ['conv2d_1[0][0]']            \n",
      "                                                                                                  \n",
      " BN_01 (BatchNormalization)  (None, 49, 49, 32)           128       ['maxpool2d_1[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 47, 47, 64)           18496     ['BN_01[0][0]']               \n",
      "                                                                                                  \n",
      " maxpool2d_2 (MaxPooling2D)  (None, 23, 23, 64)           0         ['conv2d_2[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 21, 21, 128)          73856     ['maxpool2d_2[0][0]']         \n",
      "                                                                                                  \n",
      " maxpool2d_3 (MaxPooling2D)  (None, 10, 10, 128)          0         ['conv2d_3[0][0]']            \n",
      "                                                                                                  \n",
      " BN_03 (BatchNormalization)  (None, 10, 10, 128)          512       ['maxpool2d_3[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 8, 8, 256)            295168    ['BN_03[0][0]']               \n",
      "                                                                                                  \n",
      " maxpool2d_4 (MaxPooling2D)  (None, 4, 4, 256)            0         ['conv2d_4[0][0]']            \n",
      "                                                                                                  \n",
      " BN_04 (BatchNormalization)  (None, 4, 4, 256)            1024      ['maxpool2d_4[0][0]']         \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 4096)                 0         ['BN_04[0][0]']               \n",
      "                                                                                                  \n",
      " FC_dense_1 (Dense)          (None, 288)                  1179936   ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " FC_dense_2 (Dense)          (None, 288)                  83232     ['FC_dense_1[0][0]']          \n",
      "                                                                                                  \n",
      " BN_05 (BatchNormalization)  (None, 288)                  1152      ['FC_dense_2[0][0]']          \n",
      "                                                                                                  \n",
      " FC_dense_3 (Dense)          (None, 288)                  83232     ['BN_05[0][0]']               \n",
      "                                                                                                  \n",
      " BN_06 (BatchNormalization)  (None, 288)                  1152      ['FC_dense_3[0][0]']          \n",
      "                                                                                                  \n",
      " FC128_dense_age_01 (Dense)  (None, 128)                  36992     ['BN_06[0][0]']               \n",
      "                                                                                                  \n",
      " BN_09 (BatchNormalization)  (None, 128)                  512       ['FC128_dense_age_01[0][0]']  \n",
      "                                                                                                  \n",
      " FC128_dense_age_02 (Dense)  (None, 128)                  16512     ['BN_09[0][0]']               \n",
      "                                                                                                  \n",
      " FC64_dense_gender (Dense)   (None, 128)                  36992     ['BN_06[0][0]']               \n",
      "                                                                                                  \n",
      " FC64_dense_age_03 (Dense)   (None, 128)                  16512     ['FC128_dense_age_02[0][0]']  \n",
      "                                                                                                  \n",
      " drop_1 (Dropout)            (None, 128)                  0         ['FC64_dense_gender[0][0]']   \n",
      "                                                                                                  \n",
      " FC64_dense_age_04 (Dense)   (None, 64)                   8256      ['FC64_dense_age_03[0][0]']   \n",
      "                                                                                                  \n",
      " FC16_dense_gender (Dense)   (None, 64)                   8256      ['drop_1[0][0]']              \n",
      "                                                                                                  \n",
      " FC32_dense_age_05 (Dense)   (None, 32)                   2080      ['FC64_dense_age_04[0][0]']   \n",
      "                                                                                                  \n",
      " BN_07 (BatchNormalization)  (None, 64)                   256       ['FC16_dense_gender[0][0]']   \n",
      "                                                                                                  \n",
      " BN_12 (BatchNormalization)  (None, 32)                   128       ['FC32_dense_age_05[0][0]']   \n",
      "                                                                                                  \n",
      " gender_output (Dense)       (None, 1)                    65        ['BN_07[0][0]']               \n",
      "                                                                                                  \n",
      " age_output (Dense)          (None, 10)                   330       ['BN_12[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1865675 (7.12 MB)\n",
      "Trainable params: 1863243 (7.11 MB)\n",
      "Non-trainable params: 2432 (9.50 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "best_model.summary()\n",
    "#plot_model(best_model, show_shapes=True)\n",
    "best_model.save(result_filepath+\"best_model.h5\",)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
